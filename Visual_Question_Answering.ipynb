{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Easy-VQA**"
      ],
      "metadata": {
        "id": "VCSpTHzPOk1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RESNET,BERT\n",
        "\n",
        "* No data augmentation\n",
        "* 4 layers\n",
        "* 10 epochs"
      ],
      "metadata": {
        "id": "LuvxaMQEg0z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset loading and setup"
      ],
      "metadata": {
        "id": "KAezE4oXONm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install easy-vqa\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import easy-vqa dataset\n",
        "from easy_vqa import get_train_questions, get_test_questions\n",
        "from easy_vqa import get_train_image_paths, get_test_image_paths\n",
        "\n",
        "print('Importing done')\n",
        "# -----------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------\n",
        "# Get easy-vqa data\n",
        "train_questions, train_answers, train_image_ids = get_train_questions()\n",
        "test_questions, test_answers, test_image_ids = get_test_questions()\n",
        "train_image_paths = get_train_image_paths()\n",
        "test_image_paths = get_test_image_paths()\n",
        "\n",
        "# Create label map from unique answers\n",
        "unique_answers = list(set(train_answers))\n",
        "label_map = {ans: i for i, ans in enumerate(unique_answers)}\n",
        "num_classes = len(label_map)\n",
        "print('Configuration done')\n",
        "\n",
        "# -----------------------------\n",
        "# DATASET CLASS\n",
        "# -----------------------------\n",
        "class VQADataset(Dataset):\n",
        "    def __init__(self, questions, answers, image_ids, image_paths, label_map, transform, tokenizer, max_len=20):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.image_ids = image_ids\n",
        "        self.image_paths = image_paths\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        answer = self.answers[idx]\n",
        "        image_id = self.image_ids[idx]\n",
        "        image_path = self.image_paths[image_id]\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            question,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = self.label_map[answer]\n",
        "\n",
        "        return image, encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0), torch.tensor(label)\n",
        "\n",
        "# -----------------------------\n",
        "# FINETUNABLE RESNET\n",
        "# -----------------------------\n",
        "def get_finetunable_resnet50():\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    for p in resnet.parameters():\n",
        "        p.requires_grad = False\n",
        "    for name, param in resnet.named_parameters():\n",
        "        if \"layer4\" in name:\n",
        "            param.requires_grad = True\n",
        "    resnet.fc = nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "# -----------------------------\n",
        "# FINETUNABLE BERT\n",
        "# -----------------------------\n",
        "def get_finetunable_bert():\n",
        "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    for p in bert.parameters():\n",
        "        p.requires_grad = False\n",
        "    for name, param in bert.named_parameters():\n",
        "        if any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "            param.requires_grad = True\n",
        "    return bert\n",
        "\n",
        "# -----------------------------\n",
        "# MULTIMODAL TRANSFORMER\n",
        "# -----------------------------\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=4, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.resnet = get_finetunable_resnet50()\n",
        "        self.bert = get_finetunable_bert()\n",
        "\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 64, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "\n",
        "        txt_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long),\n",
        "            torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])\n"
      ],
      "metadata": {
        "id": "WTp10lcTg75R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "I1rkjecXOWcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Starting the training')\n",
        "\n",
        "# -----------------------------\n",
        "# MAIN TRAINING LOOP + EVALUATION\n",
        "# -----------------------------\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = VQADataset(\n",
        "    train_questions, train_answers, train_image_ids, train_image_paths,\n",
        "    label_map, transform, tokenizer\n",
        ")\n",
        "test_dataset = VQADataset(\n",
        "    test_questions, test_answers, test_image_ids, test_image_paths,\n",
        "    label_map, transform, tokenizer\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Initialize model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VQAFusionTransformer(num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):  # Reduced epochs for demo\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "print('Training done')\n"
      ],
      "metadata": {
        "id": "0UJvWOtdOXUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "XcNOX2RcOID0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# EVALUATION LOOP ON TEST SET\n",
        "# -----------------------------\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu()\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "# Compute metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n",
        "\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "print(f\"F1 Score (macro): {f1_macro:.4f}\")\n",
        "print(f\"F1 Score (weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "# Full classification report\n",
        "target_names = list(label_map.keys())\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names, cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CcmUDhi4OD7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MobilenetV2,DistilBert\n",
        "\n",
        "* Image augmentation\n",
        "* 20 epochs"
      ],
      "metadata": {
        "id": "SqpBByuZk1e1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZedicI4OdYJ"
      },
      "outputs": [],
      "source": [
        "!pip install easy_vqa\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from easy_vqa import get_train_questions, get_test_questions, get_train_image_paths, get_test_image_paths\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# -----------------------------\n",
        "# CONFIGURATION FOR FULL DATASET\n",
        "# -----------------------------\n",
        "config = {\n",
        "    'batch_size': 128,  # Larger batch size for full dataset\n",
        "    'lr': 3e-5,\n",
        "    'epochs': 20,\n",
        "    'hidden_size': 256,\n",
        "    'num_workers': 4,\n",
        "    'image_size': 128,  # Kept small for speed\n",
        "    'validate_every': 2,  # Validate every 2 epochs\n",
        "    'max_seq_length': 20  # For tokenizer\n",
        "}\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# -----------------------------\n",
        "# FULL DATA LOADING\n",
        "# -----------------------------\n",
        "print(\"Loading full dataset...\")\n",
        "train_questions, train_answers, train_image_ids = get_train_questions()\n",
        "test_questions, test_answers, test_image_ids = get_test_questions()\n",
        "train_image_paths = get_train_image_paths()\n",
        "test_image_paths = get_test_image_paths()\n",
        "\n",
        "# Create label map from all answers\n",
        "unique_answers = sorted(list(set(train_answers + test_answers)))\n",
        "label_map = {ans: i for i, ans in enumerate(unique_answers)}\n",
        "num_classes = len(label_map)\n",
        "print(f\"Training samples: {len(train_questions)}\")\n",
        "print(f\"Test samples: {len(test_questions)}\")\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "\n",
        "# -----------------------------\n",
        "# OPTIMIZED DATASET CLASS\n",
        "# -----------------------------\n",
        "class EasyVQADataset(Dataset):\n",
        "    def __init__(self, questions, answers, image_ids, image_paths, label_map, transform, tokenizer, max_len=20):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "        self.image_ids = image_ids\n",
        "        self.image_paths = image_paths\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[self.image_ids[idx]]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Tokenize on-the-fly (more memory efficient)\n",
        "        encoding = self.tokenizer(\n",
        "            self.questions[idx],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            image,\n",
        "            encoding['input_ids'].squeeze(0),\n",
        "            encoding['attention_mask'].squeeze(0),\n",
        "            torch.tensor(self.label_map[self.answers[idx]])\n",
        "        )\n",
        "\n",
        "# -----------------------------\n",
        "# LIGHTWEIGHT MODEL ARCHITECTURE\n",
        "# -----------------------------\n",
        "class FastEasyVQAModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        # Lightweight image encoder (MobileNetV2)\n",
        "        self.img_encoder = models.mobilenet_v2(pretrained=True)\n",
        "        self.img_encoder.classifier = nn.Identity()  # Remove final layer\n",
        "\n",
        "        # Freeze early layers, unfreeze last few\n",
        "        for param in self.img_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.img_encoder.features[-4:].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Lightweight text encoder (DistilBERT)\n",
        "        self.txt_encoder = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        # Freeze most layers, unfreeze last layer\n",
        "        for param in self.txt_encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.txt_encoder.transformer.layer[-1].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "        # Efficient fusion and classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(1280 + 768, 512),  # MobileNetV2 (1280) + DistilBERT (768)\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        # Get image features\n",
        "        img_features = self.img_encoder(images)\n",
        "\n",
        "        # Get text features (using CLS token)\n",
        "        txt_outputs = self.txt_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_features = txt_outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Concatenate and classify\n",
        "        combined = torch.cat([img_features, txt_features], dim=1)\n",
        "        return self.classifier(combined)\n",
        "\n",
        "# -----------------------------\n",
        "# TRAINING SETUP\n",
        "# -----------------------------\n",
        "print(\"Setting up training...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Image transforms with augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((config['image_size'], config['image_size'])),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Test transforms without augmentation\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((config['image_size'], config['image_size'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = EasyVQADataset(\n",
        "    train_questions, train_answers, train_image_ids, train_image_paths,\n",
        "    label_map, transform, tokenizer, max_len=config['max_seq_length']\n",
        ")\n",
        "test_dataset = EasyVQADataset(\n",
        "    test_questions, test_answers, test_image_ids, test_image_paths,\n",
        "    label_map, test_transform, tokenizer, max_len=config['max_seq_length']\n",
        ")\n",
        "\n",
        "# Optimized data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=config['num_workers'],\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=False,\n",
        "    num_workers=config['num_workers'],\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "model = FastEasyVQAModel(num_classes=num_classes).to(device)\n",
        "\n",
        "# Optimizer with weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=config['lr'],\n",
        "    weight_decay=0.01\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Mixed precision training\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max', factor=0.5, patience=2, verbose=True\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# TRAINING LOOP\n",
        "# -----------------------------\n",
        "print(f\"Training on {len(train_questions)} samples...\")\n",
        "best_acc = 0.0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['epochs']}\")\n",
        "\n",
        "    for images, input_ids, attn_mask, labels in progress_bar:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        input_ids = input_ids.to(device, non_blocking=True)\n",
        "        attn_mask = attn_mask.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Mixed precision forward pass\n",
        "        with autocast():\n",
        "            outputs = model(images, input_ids, attn_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Backward pass with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validate only every N epochs\n",
        "    if (epoch + 1) % config['validate_every'] == 0 or (epoch + 1) == config['epochs']:\n",
        "        model.eval()\n",
        "        val_loss, correct = 0.0, 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, input_ids, attn_mask, labels in test_loader:\n",
        "                images = images.to(device, non_blocking=True)\n",
        "                input_ids = input_ids.to(device, non_blocking=True)\n",
        "                attn_mask = attn_mask.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                outputs = model(images, input_ids, attn_mask)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(test_loader)\n",
        "        val_acc = correct / len(test_dataset)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}: \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "              f\"Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_easyvqa_model.pth')\n",
        "            print(\"Saved new best model!\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.linspace(0, config['epochs'], len(val_accuracies)), val_accuracies, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# COMPREHENSIVE EVALUATION\n",
        "# -----------------------------\n",
        "print(\"\\nFinal Evaluation:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=list(label_map.keys())))\n",
        "\n",
        "# Confusion matrix\n",
        "plt.figure(figsize=(15, 12))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_map.keys(),\n",
        "            yticklabels=label_map.keys(), cmap='Blues')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix - Full EasyVQA Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# PREDICTION VISUALIZATION\n",
        "# -----------------------------\n",
        "def visualize_predictions(model, tokenizer, transform, test_data, image_paths, label_map, num_samples=5):\n",
        "    model.eval()\n",
        "    reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "    # Select random samples\n",
        "    if len(test_data) > num_samples:\n",
        "        indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
        "        test_data = [test_data[i] for i in indices]\n",
        "\n",
        "    plt.figure(figsize=(15, 3*num_samples))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, item in enumerate(test_data):\n",
        "            img_path = image_paths[item[\"image_id\"]]\n",
        "            try:\n",
        "                # Load and process image\n",
        "                image = Image.open(img_path).convert(\"RGB\")\n",
        "                display_image = image.copy()\n",
        "                image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "                # Tokenize question\n",
        "                encoding = tokenizer(\n",
        "                    item[\"question\"],\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    max_length=config['max_seq_length'],\n",
        "                    return_tensors=\"pt\"\n",
        "                ).to(device)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(image, encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
        "                probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "                confidence, pred_id = torch.max(probs, dim=1)\n",
        "                pred_label = reverse_label_map[pred_id.item()]\n",
        "\n",
        "                # Get top predictions\n",
        "                top3_probs, top3_ids = torch.topk(probs, 3)\n",
        "                top3_labels = [reverse_label_map[idx.item()] for idx in top3_ids[0]]\n",
        "                top3_confs = [f\"{prob.item():.1%}\" for prob in top3_probs[0]]\n",
        "\n",
        "                # Display results\n",
        "                plt.subplot(num_samples, 2, 2*i+1)\n",
        "                plt.imshow(display_image)\n",
        "                plt.title(f\"Image {i+1}\")\n",
        "                plt.axis('off')\n",
        "\n",
        "                plt.subplot(num_samples, 2, 2*i+2)\n",
        "                plt.text(0.1, 0.9, f\"Q: {item['question']}\", fontsize=10)\n",
        "                plt.text(0.1, 0.7, f\"Predicted: {pred_label} ({confidence.item():.1%})\",\n",
        "                         fontsize=10, color='green')\n",
        "                plt.text(0.1, 0.5, f\"Actual: {item['answer']}\", fontsize=10, color='blue')\n",
        "                plt.text(0.1, 0.3, \"Top Predictions:\", fontsize=10)\n",
        "                for j in range(3):\n",
        "                    plt.text(0.15, 0.2-0.1*j,\n",
        "                             f\"{j+1}. {top3_labels[j]} ({top3_confs[j]})\",\n",
        "                             fontsize=9)\n",
        "                plt.axis('off')\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {img_path}: {str(e)}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Prepare test data with answers\n",
        "test_data_with_answers = [{\n",
        "    \"image_id\": test_image_ids[i],\n",
        "    \"question\": test_questions[i],\n",
        "    \"answer\": test_answers[i]\n",
        "} for i in range(len(test_questions))]\n",
        "\n",
        "# Visualize predictions\n",
        "print(\"\\nVisualizing predictions...\")\n",
        "visualize_predictions(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    transform=test_transform,\n",
        "    test_data=test_data_with_answers,\n",
        "    image_paths=test_image_paths,\n",
        "    label_map=label_map,\n",
        "    num_samples=5\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# MODEL SAVING\n",
        "# -----------------------------\n",
        "def save_full_model(model, tokenizer, label_map, config):\n",
        "    \"\"\"Save all components needed for deployment\"\"\"\n",
        "    import pickle\n",
        "\n",
        "    # Save model weights\n",
        "    torch.save(model.state_dict(), 'easyvqa_full_model.pth')\n",
        "\n",
        "    # Save tokenizer\n",
        "    tokenizer.save_pretrained('./easyvqa_tokenizer')\n",
        "\n",
        "    # Save metadata\n",
        "    with open('easyvqa_metadata.pkl', 'wb') as f:\n",
        "        pickle.dump({\n",
        "            'label_map': label_map,\n",
        "            'config': config,\n",
        "            'image_size': config['image_size'],\n",
        "            'max_seq_length': config['max_seq_length']\n",
        "        }, f)\n",
        "\n",
        "    print(\"\\nModel artifacts saved:\")\n",
        "    print(\"- Model weights: easyvqa_full_model.pth\")\n",
        "    print(\"- Tokenizer: easyvqa_tokenizer/\")\n",
        "    print(\"- Metadata: easyvqa_metadata.pkl\")\n",
        "\n",
        "save_full_model(model, tokenizer, label_map, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Path-VQA"
      ],
      "metadata": {
        "id": "tTE2yMaxPC6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset loading and setup"
      ],
      "metadata": {
        "id": "8SIPY76VPGOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import io\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import dask.dataframe as dd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import transforms,models\n",
        "from transformers import BertTokenizer,BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,classification_report\n",
        "\n",
        "print('Importing done')\n",
        "\n",
        "splits = {\n",
        "    'train': 'data/train-*-of-*.parquet',\n",
        "    'validation': 'data/validation-*-of-*.parquet',\n",
        "    'test': 'data/test-*-of-*.parquet'\n",
        "}\n",
        "\n",
        "train_df = pd.DataFrame(dd.read_parquet(\"hf://datasets/flaviagiammarino/path-vqa/\" + splits[\"train\"]).compute())\n",
        "val_df = pd.DataFrame(dd.read_parquet(\"hf://datasets/flaviagiammarino/path-vqa/\" + splits[\"validation\"]).compute())\n",
        "test_df = pd.DataFrame(dd.read_parquet(\"hf://datasets/flaviagiammarino/path-vqa/\" + splits[\"test\"]).compute())\n",
        "# -----------------------------\n",
        "# Build Unified Label Map\n",
        "# -----------------------------\n",
        "\n",
        "all_answers = pd.concat([train_df['answer'], val_df['answer'], test_df['answer']])\n",
        "print(f'Total number of answers: {len(all_answers)}')\n",
        "unique_answers = sorted(all_answers.unique())\n",
        "label_map = {ans: idx for idx, ans in enumerate(unique_answers)}\n",
        "\n",
        "print(f\"Total unique answers across splits: {len(label_map)}\")\n",
        "\n",
        "print('Label map done')\n",
        "\n",
        "# -----------------------------\n",
        "# Set Seed\n",
        "# -----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset for Path-VQA\n",
        "# -----------------------------\n",
        "class PathVQADataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(io.BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            row['question'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Datasets\n",
        "train_dataset = PathVQADataset(train_df, label_map, transform, tokenizer)\n",
        "val_dataset = PathVQADataset(val_df, label_map, transform, tokenizer)\n",
        "test_dataset = PathVQADataset(test_df, label_map, transform, tokenizer)\n",
        "\n",
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "print(\"Data preparation done\")\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "def get_finetunable_resnet50():\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    for p in resnet.parameters():\n",
        "        p.requires_grad = False\n",
        "    for name, param in resnet.named_parameters():\n",
        "        if \"layer4\" in name:\n",
        "            param.requires_grad = True\n",
        "    resnet.fc = nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "def get_finetunable_bert():\n",
        "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    for p in bert.parameters():\n",
        "        p.requires_grad = False\n",
        "    for name, param in bert.named_parameters():\n",
        "        if any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "            param.requires_grad = True\n",
        "    return bert\n",
        "\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=4, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "        self.resnet = get_finetunable_resnet50()\n",
        "        self.bert = get_finetunable_bert()\n",
        "\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "\n",
        "        txt_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long),\n",
        "            torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])"
      ],
      "metadata": {
        "id": "LhPEuMtQlwAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "KfdgfOC3O7TL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Train and Evaluate Inline\n",
        "# -----------------------------\n",
        "# Assume train_loader, val_loader, test_loader, label_map are already defined\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VQAFusionTransformer(num_classes=len(label_map)).to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "val_f1s = []\n",
        "\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience = 3\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(10):  # Track loss and accuracy per epoch\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\"):\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
        "            images = images.to(device)\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images, input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu()\n",
        "            val_preds.extend(preds.tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f} | Val Acc = {val_acc*100:.2f}% | F1 = {val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improvement = 0\n",
        "        print(\"✅ Best model updated\")\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        print(f\"⚠️ No improvement for {epochs_without_improvement} epoch(s)\")\n",
        "\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"⏹️ Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Plot training curve\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\", marker='o')\n",
        "plt.title(\"Training Loss over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([v*100 for v in val_accuracies], label=\"Val Accuracy\", marker='o')\n",
        "plt.plot([v*100 for v in val_f1s], label=\"Val F1 (Macro)\", marker='x')\n",
        "plt.title(\"Validation Accuracy & F1\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"%\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the best mode\n",
        "if best_model_state is not None:\n",
        "    torch.save(best_model_state, \"best_vqa_model.pth\")\n",
        "    print(\"✅ Model saved to 'best_vqa_model.pth'\")\n"
      ],
      "metadata": {
        "id": "NJfX9tomO8aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "5tcjy4agPACA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model\n",
        "model.load_state_dict(best_model_state)\n",
        "\n",
        "# Final Evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images = images.to(device)\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        preds = torch.argmax(outputs, dim=1).cpu()\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
        "f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Invert label_map to get index -> answer\n",
        "inv_label_map = {v: k for k, v in label_map.items()}\n",
        "\n",
        "# Extract all unique labels present in test\n",
        "present_labels = sorted(set(all_preds + all_labels))\n",
        "\n",
        "# Debug print\n",
        "print(f\"Predicted/Test Labels: {len(present_labels)}\")\n",
        "print(f\"Total label_map classes: {len(label_map)}\")\n",
        "\n",
        "# Now compute classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(\n",
        "    all_labels,\n",
        "    all_preds,\n",
        "    labels=present_labels,\n",
        "    target_names=[inv_label_map[i] for i in present_labels]\n",
        "))\n",
        "\n",
        "print(f\"\\nTest Accuracy: {acc*100:.2f}%\")\n",
        "print(f\"F1 Macro: {f1_macro:.4f} | F1 Weighted: {f1_weighted:.4f}\")\n",
        "from collections import Counter\n",
        "\n",
        "# Count top classes in predictions and labels\n",
        "combined = all_preds + all_labels\n",
        "top_classes = [item for item, _ in Counter(combined).most_common(10)]\n",
        "\n",
        "# Filter predictions and labels to only include top classes\n",
        "filtered_preds = [p for p, t in zip(all_preds, all_labels) if p in top_classes and t in top_classes]\n",
        "filtered_labels = [t for p, t in zip(all_preds, all_labels) if p in top_classes and t in top_classes]\n",
        "\n",
        "# Get the confusion matrix\n",
        "cm = confusion_matrix(filtered_labels, filtered_preds, labels=top_classes)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True,\n",
        "            xticklabels=[inv_label_map[i] for i in top_classes],\n",
        "            yticklabels=[inv_label_map[i] for i in top_classes],\n",
        "            cmap='Blues', fmt='g')\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix (Top 10 Classes Only)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-S2W33PIPBJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VQA-RAD\n",
        "\n",
        "With grad-cam\n",
        "\n",
        "From : vqa.ipynb"
      ],
      "metadata": {
        "id": "w3GiiDMrnH-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset loading and setup"
      ],
      "metadata": {
        "id": "zRUmBFKDPrz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import random\n",
        "import io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-eb8844602202be60.parquet',\n",
        "    'test': 'data/test-00000-of-00001-e5bc3d208bb4deeb.parquet'\n",
        "}\n",
        "\n",
        "train_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"train\"])\n",
        "test_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"test\"])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "all_answers = pd.concat([train_df['answer'], val_df['answer'], test_df['answer']])\n",
        "label_map_vqarad = {ans: idx for idx, ans in enumerate(sorted(all_answers.unique()))}\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Class\n",
        "# -----------------------------\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(io.BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            row['question'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms and Tokenizer\n",
        "# -----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_dataset = VQARADDataset(train_df, label_map_vqarad, transform, tokenizer)\n",
        "val_dataset = VQARADDataset(val_df, label_map_vqarad, transform, tokenizer)\n",
        "test_dataset = VQARADDataset(test_df, label_map_vqarad, transform, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model Definition\n",
        "# -----------------------------\n",
        "def get_finetunable_resnet50():\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    for p in resnet.parameters(): p.requires_grad = False\n",
        "    for name, param in resnet.named_parameters():\n",
        "        if \"layer4\" in name:\n",
        "            param.requires_grad = True\n",
        "    resnet.fc = nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "def get_finetunable_bert():\n",
        "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    for p in bert.parameters(): p.requires_grad = False\n",
        "    for name, param in bert.named_parameters():\n",
        "        if any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "            param.requires_grad = True\n",
        "    return bert\n",
        "\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=8, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.resnet = get_finetunable_resnet50()\n",
        "        self.bert = get_finetunable_bert()\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "        txt_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long), torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])"
      ],
      "metadata": {
        "id": "Knd5p7pZlxDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "bzFWIxPjPZ8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "# Load pretrained weights (excluding mismatched layers) on GPU\n",
        "def load_pretrained_weights(model, pretrained_path, device='cuda'):\n",
        "    print(f\"🔄 Loading pretrained model from: {pretrained_path}\")\n",
        "\n",
        "    # Load the full checkpoint to CPU first\n",
        "    pretrained_dict = torch.load(pretrained_path, map_location=device)\n",
        "\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # Filter out keys that mismatch in shape (like final classification layer)\n",
        "    filtered_dict = {\n",
        "        k: v for k, v in pretrained_dict.items()\n",
        "        if k in model_dict and model_dict[k].shape == v.shape\n",
        "    }\n",
        "\n",
        "    print(f\"✅ {len(filtered_dict)}/{len(pretrained_dict)} parameters loaded from pretrained model.\")\n",
        "\n",
        "    # Update and load the state_dict\n",
        "    model_dict.update(filtered_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Example usage\n",
        "model = VQAFusionTransformer(num_classes=len(label_map_vqarad)).to(device)\n",
        "load_pretrained_weights(model, \"/content/drive/MyDrive/My Folder/best_vqa_model.pth\", device=device)\n",
        "print(\"✅ Loaded PathVQA pre-trained model.\")\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "\n",
        "train_losses, val_accuracies, val_f1s = [], [], []\n",
        "best_val_acc = 0.0\n",
        "best_model_state = None\n",
        "patience = 10\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(80):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_losses.append(total_loss / len(train_loader))\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "    print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f} | Val Acc={val_acc*100:.2f}% | F1={val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "epochs_ran = len(train_losses)\n",
        "\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs_ran + 1), train_losses, label='Train Loss', marker='o', color='red')\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy & F1\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs_ran + 1), [v * 100 for v in val_accuracies], label='Val Accuracy (%)', marker='s', color='blue')\n",
        "plt.plot(range(1, epochs_ran + 1), [f * 100 for f in val_f1s], label='Val F1 Macro (%)', marker='x', color='green')\n",
        "plt.title('Validation Accuracy and F1')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Percentage')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i8P5Lr4gPa5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "12TEJHUSPbT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#testing\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Test\"):\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "        test_preds.extend(preds.cpu().tolist())\n",
        "        test_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, test_preds) * 100)\n",
        "print(\"Test F1 (macro):\", f1_score(test_labels, test_preds, average='macro') * 100)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds))\n",
        "class GradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_input, grad_output):\n",
        "            self.gradients = grad_output[0].detach()\n",
        "\n",
        "        self.hook_handles.append(self.target_layer.register_forward_hook(forward_hook))\n",
        "        self.hook_handles.append(self.target_layer.register_full_backward_hook(backward_hook))\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "    def generate(self, image_tensor, input_ids, attention_mask, class_idx=None):\n",
        "        # Ensure we're in eval mode but with gradients enabled\n",
        "        self.model.eval()\n",
        "\n",
        "        # Prepare inputs\n",
        "        image_tensor = image_tensor.unsqueeze(0).to(device).requires_grad_()\n",
        "        input_ids = input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "\n",
        "        # Forward pass with gradients\n",
        "        with torch.set_grad_enabled(True):\n",
        "            output = self.model(image_tensor, input_ids, attention_mask)\n",
        "\n",
        "            if class_idx is None:\n",
        "                class_idx = output.argmax(dim=1).item()\n",
        "\n",
        "            # Zero gradients\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            # Backward pass for specific class\n",
        "            one_hot = torch.zeros_like(output)\n",
        "            one_hot[0, class_idx] = 1\n",
        "            output.backward(gradient=one_hot, retain_graph=True)\n",
        "\n",
        "        # Process gradients and activations\n",
        "        gradients = self.gradients\n",
        "        activations = self.activations\n",
        "\n",
        "        # Pool gradients and weight activations\n",
        "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
        "        for i in range(activations.shape[1]):\n",
        "            activations[:, i, :, :] *= pooled_gradients[i]\n",
        "\n",
        "        # Generate heatmap\n",
        "        heatmap = torch.mean(activations, dim=1).squeeze()\n",
        "        heatmap = torch.relu(heatmap)  # Apply ReLU\n",
        "        heatmap = heatmap / torch.max(heatmap)  # Normalize\n",
        "\n",
        "        return heatmap.cpu().numpy(), class_idx\n",
        "\n",
        "# Visualization function\n",
        "def show_gradcam_comparison(img_tensor, heatmap, gt_label, pred_label, is_correct):\n",
        "    import cv2\n",
        "    import matplotlib.cm as cm\n",
        "\n",
        "    img = img_tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
        "    heatmap_colored = cm.jet(heatmap)[..., :3]\n",
        "    superimposed_img = heatmap_colored * 0.5 + img * 0.5\n",
        "\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Original Image\\nGT: {gt_label}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Grad-CAM\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(superimposed_img)\n",
        "    plt.title(f\"Grad-CAM\\nPred: {pred_label}\\n{'CORRECT' if is_correct else 'INCORRECT'}\",\n",
        "             color='green' if is_correct else 'red')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main visualization code\n",
        "grad_cam = GradCAM(model, model.resnet.layer4)\n",
        "\n",
        "num_samples = 3\n",
        "correct_count = 0\n",
        "incorrect_count = 0\n",
        "\n",
        "model.eval()\n",
        "for idx, (img, input_ids, attention_mask, label) in enumerate(test_dataset):\n",
        "    if correct_count >= num_samples and incorrect_count >= num_samples:\n",
        "        break\n",
        "\n",
        "    # Get prediction\n",
        "    with torch.no_grad():\n",
        "        img_tensor = img.unsqueeze(0).to(device)\n",
        "        input_ids_tensor = input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask_tensor = attention_mask.unsqueeze(0).to(device)\n",
        "\n",
        "        output = model(img_tensor, input_ids_tensor, attention_mask_tensor)\n",
        "        pred_class = output.argmax(dim=1).item()\n",
        "\n",
        "    is_correct = (pred_class == label.item())\n",
        "\n",
        "    # Skip if we have enough samples\n",
        "    if is_correct and correct_count >= num_samples:\n",
        "        continue\n",
        "    if not is_correct and incorrect_count >= num_samples:\n",
        "        continue\n",
        "\n",
        "    # Generate Grad-CAM\n",
        "    heatmap, _ = grad_cam.generate(img, input_ids, attention_mask)\n",
        "\n",
        "    # Get label names\n",
        "    gt_label_name = list(label_map_vqarad.keys())[list(label_map_vqarad.values()).index(label.item())]\n",
        "    pred_label_name = list(label_map_vqarad.keys())[pred_class]\n",
        "\n",
        "    # Show visualization\n",
        "    show_gradcam_comparison(img, heatmap, gt_label_name, pred_label_name, is_correct)\n",
        "\n",
        "    # Update counters\n",
        "    if is_correct:\n",
        "        correct_count += 1\n",
        "    else:\n",
        "        incorrect_count += 1\n",
        "\n",
        "grad_cam.remove_hooks()"
      ],
      "metadata": {
        "id": "-fmx7bEdPcw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid-search"
      ],
      "metadata": {
        "id": "QWjb5DmqonIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from PIL import Image\n",
        "import io, random, numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Set seed and device\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 2. Load pretrained weights correctly\n",
        "def load_pretrained_weights(model, path, device):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model_dict = model.state_dict()\n",
        "    filtered = {k: v.to(device) for k, v in checkpoint.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
        "    print(f\"Loaded {len(filtered)}/{len(model_dict)} parameters.\")\n",
        "    model_dict.update(filtered)\n",
        "    model.load_state_dict(model_dict)\n",
        "    model.to(device)\n",
        "\n",
        "# 3. Dataset Class\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(io.BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        tokens = self.tokenizer(row['question'], padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "\n",
        "# 4. Model\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=4, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes))\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "        txt_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "        mod_ids = torch.cat([torch.ones((B, 1), dtype=torch.long), torch.zeros((B, input_ids.size(1)), dtype=torch.long)], dim=1).to(images.device)\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])\n",
        "\n",
        "# 5. Training loop\n",
        "\n",
        "def train_vqa_model_finetune_from_pathvqa(pathvqa_model_path, train_loader, val_loader, label_map, lr=1e-5,\n",
        "                                           resnet_unfreeze=False, bert_unfreeze=False, tf_layers=4):\n",
        "    model = VQAFusionTransformer(num_classes=len(label_map), num_layers=tf_layers).to(device)\n",
        "    load_pretrained_weights(model, pathvqa_model_path, device)\n",
        "\n",
        "    # Unfreeze layers as needed\n",
        "    for name, param in model.resnet.named_parameters():\n",
        "        if resnet_unfreeze and \"layer4\" in name:\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    for name, param in model.bert.named_parameters():\n",
        "        if bert_unfreeze and any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "            param.requires_grad = True\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.train()\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in val_loader:\n",
        "            images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    acc = accuracy_score(val_labels, val_preds)\n",
        "    f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "# 6. Grid Search Example\n",
        "learning_rates = [1e-5, 3e-5]\n",
        "resnet_opts = [False, True]\n",
        "bert_opts = [False, True]\n",
        "transformer_layers = [2, 4]\n",
        "\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for resnet_unfreeze in resnet_opts:\n",
        "        for bert_unfreeze in bert_opts:\n",
        "            for tf_layer in transformer_layers:\n",
        "                print(f\"\\n🔧 Running: lr={lr}, resnet={resnet_unfreeze}, bert={bert_unfreeze}, layers={tf_layer}\")\n",
        "                acc, f1 = train_vqa_model_finetune_from_pathvqa(\n",
        "                    pathvqa_model_path=\"/content/drive/MyDrive/vqa_models/best_vqa_model.pth\",\n",
        "                    train_loader=train_loader, val_loader=val_loader,\n",
        "                    label_map=label_map_vqarad, lr=lr,\n",
        "                    resnet_unfreeze=resnet_unfreeze, bert_unfreeze=bert_unfreeze,\n",
        "                    tf_layers=tf_layer\n",
        "                )\n",
        "                results.append({\n",
        "                    \"lr\": lr,\n",
        "                    \"resnet_unfreeze\": resnet_unfreeze,\n",
        "                    \"bert_unfreeze\": bert_unfreeze,\n",
        "                    \"tf_layers\": tf_layer,\n",
        "                    \"val_acc\": acc,\n",
        "                    \"val_f1\": f1\n",
        "                })\n",
        "                print(f\"✅ Done: Acc={acc:.4f}, F1={f1:.4f}\")\n",
        "\n",
        "import pandas as pd\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df.sort_values(by=\"val_acc\", ascending=False))\n"
      ],
      "metadata": {
        "id": "-vpyAZ_WoIPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bio-BERT"
      ],
      "metadata": {
        "id": "ysqXdn0Qooy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import AutoTokenizer, AutoModel  # For BioBERT\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Loading\n",
        "# -----------------------------\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-eb8844602202be60.parquet',\n",
        "    'test': 'data/test-00000-of-00001-e5bc3d208bb4deeb.parquet'\n",
        "}\n",
        "\n",
        "train_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"train\"])\n",
        "test_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"test\"])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "all_answers = pd.concat([train_df['answer'], val_df['answer'], test_df['answer']])\n",
        "label_map_vqarad = {ans: idx for idx, ans in enumerate(sorted(all_answers.unique()))}\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Class\n",
        "# -----------------------------\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(io.BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            row['question'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms & Tokenizer\n",
        "# -----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "train_dataset = VQARADDataset(train_df, label_map_vqarad, transform, tokenizer)\n",
        "val_dataset = VQARADDataset(val_df, label_map_vqarad, transform, tokenizer)\n",
        "test_dataset = VQARADDataset(test_df, label_map_vqarad, transform, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model: ResNet + BioBERT + Transformer\n",
        "# -----------------------------\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=8, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        for name, param in self.resnet.named_parameters():\n",
        "            if \"layer4\" in name:\n",
        "                param.requires_grad = True\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        self.biobert = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "        for param in self.biobert.parameters():\n",
        "            param.requires_grad = False\n",
        "        for name, param in self.biobert.named_parameters():\n",
        "            if any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "                param.requires_grad = True\n",
        "\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "\n",
        "        txt_out = self.biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long),\n",
        "            torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])\n",
        "\n",
        "# -----------------------------\n",
        "# Training + Evaluation\n",
        "# -----------------------------\n",
        "model = VQAFusionTransformer(num_classes=len(label_map_vqarad)).to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "best_model_state = None\n",
        "train_losses, val_accuracies, val_f1s = [], [], []\n",
        "best_val_acc, patience, epochs_without_improvement = 0.0, 5, 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_losses.append(total_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in val_loader:\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f} | Val Acc={val_acc*100:.2f}% | F1={val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Save\n",
        "torch.save(best_model_state, \"vqarad_biobert_finetuned_model.pth\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test Evaluation\n",
        "# -----------------------------\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "        test_preds.extend(preds.cpu().tolist())\n",
        "        test_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, test_preds) * 100)\n",
        "print(\"Test F1 (macro):\", f1_score(test_labels, test_preds, average='macro') * 100)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds))\n"
      ],
      "metadata": {
        "id": "ZasOIpSxosw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Clinical-BERT"
      ],
      "metadata": {
        "id": "0_WKqo9JotNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, io\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from transformers import AutoTokenizer, AutoModel  # For BioBERT\n",
        "\n",
        "# -----------------------------\n",
        "# Setup\n",
        "# -----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Loading\n",
        "# -----------------------------\n",
        "splits = {\n",
        "    'train': 'data/train-00000-of-00001-eb8844602202be60.parquet',\n",
        "    'test': 'data/test-00000-of-00001-e5bc3d208bb4deeb.parquet'\n",
        "}\n",
        "\n",
        "train_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"train\"])\n",
        "test_df = pd.read_parquet(\"hf://datasets/flaviagiammarino/vqa-rad/\" + splits[\"test\"])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "all_answers = pd.concat([train_df['answer'], val_df['answer'], test_df['answer']])\n",
        "label_map_vqarad = {ans: idx for idx, ans in enumerate(sorted(all_answers.unique()))}\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Class\n",
        "# -----------------------------\n",
        "class VQARADDataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(io.BytesIO(row['image']['bytes'])).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = self.tokenizer(\n",
        "            row['question'],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms & Tokenizer\n",
        "# -----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "\n",
        "train_dataset = VQARADDataset(train_df, label_map_vqarad, transform, tokenizer)\n",
        "val_dataset = VQARADDataset(val_df, label_map_vqarad, transform, tokenizer)\n",
        "test_dataset = VQARADDataset(test_df, label_map_vqarad, transform, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model: ResNet + BioBERT + Transformer\n",
        "# -----------------------------\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=8, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        for name, param in self.resnet.named_parameters():\n",
        "            if \"layer4\" in name:\n",
        "                param.requires_grad = True\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        self.biobert = AutoModel.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
        "        for param in self.biobert.parameters():\n",
        "            param.requires_grad = False\n",
        "        for name, param in self.biobert.named_parameters():\n",
        "            if any(f\"encoder.layer.{i}\" in name for i in range(8, 12)):\n",
        "                param.requires_grad = True\n",
        "\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "\n",
        "        txt_out = self.biobert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long),\n",
        "            torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])\n",
        "\n",
        "# -----------------------------\n",
        "# Training + Evaluation\n",
        "# -----------------------------\n",
        "model = VQAFusionTransformer(num_classes=len(label_map_vqarad)).to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "best_model_state = None\n",
        "train_losses, val_accuracies, val_f1s = [], [], []\n",
        "best_val_acc, patience, epochs_without_improvement = 0.0, 5, 0\n",
        "\n",
        "for epoch in range(30):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_losses.append(total_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in val_loader:\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f} | Val Acc={val_acc*100:.2f}% | F1={val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Save\n",
        "torch.save(best_model_state, \"vqarad_biobert_finetuned_model.pth\")\n",
        "\n",
        "# -----------------------------\n",
        "# Test Evaluation\n",
        "# -----------------------------\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Testing\"):\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "        test_preds.extend(preds.cpu().tolist())\n",
        "        test_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, test_preds) * 100)\n",
        "print(\"Test F1 (macro):\", f1_score(test_labels, test_preds, average='macro') * 100)\n",
        "print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds))\n"
      ],
      "metadata": {
        "id": "DTdjNtn-ouR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Botany-VQA"
      ],
      "metadata": {
        "id": "kbea6NR8yMU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset loading and setup"
      ],
      "metadata": {
        "id": "y88j3c3JQJZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 1: Download Oxford Flowers 102 dataset\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat\n",
        "\n",
        "# Step 2: Extract images\n",
        "!tar -xvzf 102flowers.tgz\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# -----------------------------\n",
        "# Set seed\n",
        "# -----------------------------\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# -----------------------------\n",
        "# Read and fix your CSV\n",
        "# -----------------------------\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/botany_vqa_v1.csv\")\n",
        "#in a similar way load the 2nd version as well\n",
        "print(\"✅ Loaded CSV:\", df.shape)\n",
        "\n",
        "# Apply 5-digit zero padding fix to filenames\n",
        "def fix_image_path(img_path):\n",
        "    num = int(img_path.split('_')[-1].split('.')[0])\n",
        "    return f\"jpg/image_{num:05d}.jpg\"\n",
        "\n",
        "df['image_path'] = df['image_path'].apply(fix_image_path)\n",
        "\n",
        "# -----------------------------\n",
        "# Label mapping\n",
        "# -----------------------------\n",
        "label_map = {ans: idx for idx, ans in enumerate(sorted(df['answer'].unique()))}\n",
        "print(\"✅ Classes:\", label_map)\n",
        "\n",
        "# -----------------------------\n",
        "# Train/Val/Test Split\n",
        "# -----------------------------\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset Class\n",
        "# -----------------------------\n",
        "class BotanyVQADataset(Dataset):\n",
        "    def __init__(self, df, label_map, transform, tokenizer, max_len=64):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.label_map = label_map\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = row['image_path']\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "\n",
        "        tokens = tokenizer(\n",
        "            row['question'], padding=\"max_length\", truncation=True,\n",
        "            max_length=self.max_len, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        label = torch.tensor(self.label_map[row['answer']])\n",
        "        return img, tokens['input_ids'].squeeze(0), tokens['attention_mask'].squeeze(0), label\n",
        "\n",
        "# -----------------------------\n",
        "# Transforms and Tokenizer\n",
        "# -----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_dataset = BotanyVQADataset(train_df, label_map, transform, tokenizer)\n",
        "val_dataset = BotanyVQADataset(val_df, label_map, transform, tokenizer)\n",
        "test_dataset = BotanyVQADataset(test_df, label_map, transform, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "def get_finetunable_resnet50():\n",
        "    resnet = models.resnet50(pretrained=True)\n",
        "    for p in resnet.parameters(): p.requires_grad = True\n",
        "    resnet.fc = nn.Identity()\n",
        "    return resnet\n",
        "\n",
        "def get_finetunable_bert():\n",
        "    bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "    for p in bert.parameters(): p.requires_grad = True\n",
        "    return bert\n",
        "\n",
        "class VQAFusionTransformer(nn.Module):\n",
        "    def __init__(self, num_classes, d_model=512, num_layers=8, num_heads=8, max_len=64):\n",
        "        super().__init__()\n",
        "        self.resnet = get_finetunable_resnet50()\n",
        "        self.bert = get_finetunable_bert()\n",
        "        self.img_proj = nn.Linear(2048, d_model)\n",
        "        self.txt_proj = nn.Linear(768, d_model)\n",
        "        self.mod_embed = nn.Embedding(2, d_model)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls = nn.Sequential(\n",
        "            nn.Linear(d_model, 256), nn.ReLU(), nn.Dropout(0.3), nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        B = images.size(0)\n",
        "        img_feat = self.resnet(images)\n",
        "        img_tokens = self.img_proj(img_feat).unsqueeze(1)\n",
        "        txt_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        txt_tokens = self.txt_proj(txt_out.last_hidden_state)\n",
        "        x = torch.cat([img_tokens, txt_tokens], dim=1)\n",
        "        mod_ids = torch.cat([\n",
        "            torch.ones((B, 1), dtype=torch.long), torch.zeros((B, input_ids.size(1)), dtype=torch.long)\n",
        "        ], dim=1).to(images.device)\n",
        "        x = x + self.mod_embed(mod_ids) + self.pos_embed[:, :x.size(1), :]\n",
        "        fused = self.encoder(x)\n",
        "        return self.cls(fused[:, 0])"
      ],
      "metadata": {
        "id": "7eWKEyqxyNof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "IprQXAuiP95H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = VQAFusionTransformer(num_classes=len(label_map)).to(device)\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "train_losses, val_accuracies, val_f1s = [], [], []\n",
        "best_val_acc = 0.0\n",
        "patience = 30\n",
        "epochs_without_improvement = 0\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "        images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "        outputs = model(images, input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_losses.append(total_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    val_preds, val_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for images, input_ids, attention_mask, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "            images, input_ids, attention_mask, labels = images.to(device), input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "            preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "            val_preds.extend(preds.cpu().tolist())\n",
        "            val_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    val_acc = accuracy_score(val_labels, val_preds)\n",
        "    val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "    print(f\"Epoch {epoch+1}: Loss={train_losses[-1]:.4f} | Val Acc={val_acc*100:.2f}% | F1={val_f1:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_model_state = model.state_dict()\n",
        "        epochs_without_improvement = 0\n",
        "    else:\n",
        "        epochs_without_improvement += 1\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# -----------------------------\n",
        "# Plotting\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(14, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o', color='red')\n",
        "plt.title('Training Loss'); plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.grid(True); plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(val_accuracies) + 1), [v * 100 for v in val_accuracies], label='Val Accuracy (%)', marker='s', color='blue')\n",
        "plt.plot(range(1, len(val_f1s) + 1), [f * 100 for f in val_f1s], label='Val F1 Macro (%)', marker='x', color='green')\n",
        "plt.title('Validation Accuracy and F1'); plt.xlabel('Epoch'); plt.ylabel('Percentage'); plt.grid(True); plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hYnMQGfIP-uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Evaluation"
      ],
      "metadata": {
        "id": "v-m3LKZjP_ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# -----------------------------\n",
        "# Final Test Evaluation\n",
        "# -----------------------------\n",
        "model.load_state_dict(best_model_state)\n",
        "model.eval()\n",
        "test_preds, test_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, input_ids, attention_mask, labels in tqdm(test_loader, desc=\"Test\"):\n",
        "        images, input_ids, attention_mask = images.to(device), input_ids.to(device), attention_mask.to(device)\n",
        "        labels = labels.to(device)\n",
        "        preds = torch.argmax(model(images, input_ids, attention_mask), dim=1)\n",
        "        test_preds.extend(preds.cpu().tolist())\n",
        "        test_labels.extend(labels.cpu().tolist())\n",
        "class GradCAMExtractor:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "        self.hook_handles = []\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        target_layer = self.model.resnet.layer4\n",
        "        self.hook_handles.append(target_layer.register_forward_hook(forward_hook))\n",
        "        self.hook_handles.append(target_layer.register_backward_hook(backward_hook))\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        for handle in self.hook_handles:\n",
        "            handle.remove()\n",
        "\n",
        "    def get_gradcam(self, class_idx=None):\n",
        "        pooled_grads = torch.mean(self.gradients, dim=[0, 2, 3])  # [C]\n",
        "        for i in range(self.activations.shape[1]):\n",
        "            self.activations[:, i, :, :] *= pooled_grads[i]\n",
        "        heatmap = torch.mean(self.activations, dim=1).squeeze()\n",
        "        heatmap = torch.clamp(heatmap, min=0)\n",
        "        heatmap /= torch.max(heatmap)\n",
        "        return heatmap.cpu().numpy()\n",
        "\n",
        "print(\"Test Accuracy:\", accuracy_score(test_labels, test_preds) * 100)\n",
        "print(\"Test F1 (macro):\", f1_score(test_labels, test_preds, average='macro') * 100)\n",
        "# print(\"\\nClassification Report:\\n\", classification_report(test_labels, test_preds))\n",
        "\n",
        "import cv2\n",
        "def visualize_gradcam(model, extractor, dataset, correct=True, num=2):\n",
        "    model.eval()\n",
        "    shown = 0\n",
        "    for i in range(len(dataset)):\n",
        "        image, input_ids, attention_mask, label = dataset[i]\n",
        "        img_tensor = image.unsqueeze(0).to(device)\n",
        "        input_ids = input_ids.unsqueeze(0).to(device)\n",
        "        attention_mask = attention_mask.unsqueeze(0).to(device)\n",
        "        label = torch.tensor(label).unsqueeze(0).to(device)\n",
        "\n",
        "        extractor.model.zero_grad()\n",
        "        output = model(img_tensor, input_ids, attention_mask)\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "\n",
        "        if ((pred == label) and correct) or ((pred != label) and not correct):\n",
        "            output[0, pred].backward()\n",
        "            heatmap = extractor.get_gradcam()\n",
        "\n",
        "            img_np = image.permute(1, 2, 0).numpy()\n",
        "            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())\n",
        "\n",
        "            heatmap_resized = cv2.resize(heatmap, (224, 224))\n",
        "            heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n",
        "            superimposed = cv2.addWeighted(np.uint8(255 * img_np), 0.6, heatmap_colored, 0.4, 0)\n",
        "\n",
        "            plt.figure(figsize=(4, 4))\n",
        "            plt.title(f\"Pred: {pred.item()} | GT: {label.item()}\")\n",
        "            plt.imshow(superimposed)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            shown += 1\n",
        "            if shown >= num:\n",
        "                break\n",
        "extractor = GradCAMExtractor(model)\n",
        "\n",
        "print(\"✅ Correct Predictions\")\n",
        "visualize_gradcam(model, extractor, test_dataset, correct=True, num=2)\n",
        "\n",
        "print(\"❌ Incorrect Predictions\")\n",
        "visualize_gradcam(model, extractor, test_dataset, correct=False, num=2)\n",
        "\n",
        "extractor.remove_hooks()\n",
        "\n",
        "\n",
        "# Path in your Google Drive\n",
        "model_save_path = \"/content/drive/MyDrive/vqa_fusion_model.pth\"\n",
        "\n",
        "# Save the best model weights\n",
        "torch.save(best_model_state, model_save_path)\n",
        "\n",
        "print(f\"✅ Model saved to: {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "_kxz8XuwQAfZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}